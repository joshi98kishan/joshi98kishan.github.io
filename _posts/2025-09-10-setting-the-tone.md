---
title: Setting The Tone
layout: post
subtitle: Setting the tone of this blogsite. And how blogging will help resolve my contradiction. About my journey, AI, importance of maths. 
---


_In this blog, I state the importance of maths to learn AI multiple times but in different ways._
_Whenever I mention ML, it means I am considering both classical and DL. And in the context of this blog, ML and AI are same. I will mention AI to emphasis on the human level problem solving._

## Introduction
When I started learning "Machine Learning" in the summer of 2019, I kept learning it via online resources since then because I find it very interesting. And after some couple of weeks, I got introduced to "Deep Learning" which made me realized that researchers are trying to create the computer software (many neurons like elements connected to each other) very similar to human brain, so that computer can do human-level tasks. That idea of giving the human-level intelligence to computers - Artificial Intelligence - is so fascinating. Also the idea that one solution can impact the diverse social problems like healthcare, education, agriculture, improving science etc. But also the far fetched problems like finding habitable planets with the help of robot explorers. Since then I kept learning about AI, keeping aside the classical (non deep-learning) ML algorithms. And I set the goal of becoming the AI expert. I didn't actually defined what kind (researcher or engineer) of "AI expert" I wanna become until after my job. I just knew that I wanted to become good in AI and solve problems with it.  

### Prequisite
#### ML Knowledge Tree
#### Applied vs Fundamental Research

## How I would learn any subject?

### Disclaimer
This is especially true for my study during undergraduate years (computer science engineering) and beyond. And I could only write this in retrospect. During my undergraduate years and first half of my job, I could not really answer this question or didn't even had this question. Also I am mentioning this at the begining of this blog to make the rest of the blog easy to read. Because here I will be defining some terms which I will refer throughout the blog. And whatever I write here is atleast true for me, if not for other few people. This particular section is the only section of the blog which is not easy to grasp comparitively.

### Understanding Intuitively
I would learn anything "intuitively". Understanding intuitively means realizing that the incoming knowledge via study is just the new form of your existing knowledge. You just look at the incoming knowledge from the point of view of your existing knowledge and it feels like you already know this, nothing new in this, it is just my knowledge in the different form. Understanding a topic intuitively is fast because it easy to recognise your existing knowledge in the new form without requiring much concentration. For example, if you know how to add two numbers: x+y and you know about functions: function(x) then if you look at: f(x)+g(y), you would understand it intuitively because it is just the new form of the knowledge you already know. Hence, it also means that understanding intuitively means "comprehensive understanding" (more details in later sections but it is self-explanatory).

### Understanding intuitively fails: Hard topics
But the thing is that not all the topic can be understood intuitively because you do not have all the knowledge. Let's call such topics the "hard" topics. Understanding hard topics requires your full concentration for long enough - let's call it the "hard way" to learn anything. Called it the "hard" topic because I did not had longer attention span. So, the only _way_ to understand the hard topic is to first increase the attention span, which is also a hard feat (I explain below why it is so) - hence called it "hard way". When you learn a hard topic, then there is the _expansion of your knowledge_ and you have the comprehensive understanding of the hard topic. Also some of the other hard topics becomes easy topics for you. Without the expansion of knowledge, you cannot understand the hard topics. Hard way because _I had short attention span and it is hard for anyone to make the attention span longer_. 

### Click moment in the easy way
In contrast to this, let's call those topics which can be understood intuitively as "easy" because you have all the knowledge of that topic, you are just trying to realize (which is easy) that the given topic has which form of your knowledge. And the process of understanding any topic intuitively as "easy way" because no special concentration is required.  Understanding intuitively is not about step by step thinking but it is more of like, you look at the topic then unconscious mind starts to work. Meanwhile conscious mind staring at the topic and waiting for that "click" moment when unconscious mind returns the result, whether you have found the form of your exisiting knowledge which explains comprehensively the given topic or a form which only explains it narrowly. 

Understanding intuitively is faster and effortless than time and efforts required for expansion of the knowledge. Easy way requires no special effort, you stare at the topic and wait for the click moment. Sometimes that click moment takes time, then I will conclude that the given topic is not an "easy" one (hence, a "hard" one).  


### Hard topic the easy way, Narrow understanding
You would understand in a bit why I defined these terms. Learning the easy topics with hard way does not makes sense because the moment you bring your concentration on the topic, you would understand it intuitively. So in a way, you end up learning it the easy way. But there is a possibility of _trying_ to learn the hard topic, the easy way. Because learning the hard topic the hard way was next to impossible for me (more explanation in next para). So I would end up learning the hard topics the easy way, i.e., trying to understand intuitively. Easy way won't expand your knowledge but expansion of knowledge is must to understand hard topics. So in such situation I would end up having only "weak intuitive understanding" - that is I will find the new knowledge _closer_ to some form of my existing knowledge but no exact match. Or "narrow" understanding as opposed to comprehensive understanding. For example, if I know addition of two numbers. Now if I try to understand the addition of two vectors, in a easy way. I would only have weak intuitive understanding that addition of two vectors is similar to adding two numbers but in reality it is not exactly like that. Also, sometimes I would misunderstood that I have intuitive understanding but in reality, I only had weak intuitive understanding. 

### Killing the ego and resurrecting at will
Why it is next to impossible for me to learn the hard way, which is required for the hard topics? I had shorter attention span. Making the attention span longer is difficult for anyone. When you have a full concentration at anything, you have kind of lost yourself and the world for that time period. You are not aware of your self and anything in the world except that particular topic which means you are not having any thought apart from the thoughts require you to understand the topic. You are just thinking about that topic and trying to understand it. That is what I mean by losing your self and the world. It may sound easy but it is really not. There are thousand ways to remind of your self and the world. But only one way to lose and that one way is to think about that particular topic only. To understand the hard topics, that is, to expand your knowledge, a longer attention span is required. Means, you have to concentrate for longer period, you have to lose yourself and the world for longer period, that is the difficult I am talking about. Difficulty is that you can not make your attention span longer just like that (instantly) but it is a _long_ process of discplining yourself and conscious practice of longer attention span. 

### Harder way
If I am aware of the importance of a hard topic, then instead of working to increase the attention span, I would try to understand the hard topic right way (with shorter attention span). But understanding in such way is more difficult than increasing the attention span because that process will be extremely slow. Often, you will get stuck for long time with no progress (more details later). Hence, such a way is a "harder" way. 

### Never treaded the hard way
There is no shortcut to understand the hard topics. You have to put a lot of efforts. As I mentioned, there are thousand ways you can fail and you have to put efforts thousand times. During my college and job years, it was not easy for me to put this much of efforts and discipline. So, _I would never learn the hard topics the hard way_.


### My state of maths knowledge in the college
Now let's talk about the particular topic - MATHEMATICS. Maths was really a easy topic for me in the school. But college level maths was a hard topic for me, so I only have narrow understanding of it. I am only good at school level maths.  

    Note: ["learning the hard way" means something else in the english, but here I mean it differently, as I explained above. I called it this way because that way is hard for me.]


## About me self-learning ML

### First course in ML
During my engineering years, either I would find the topics hard or easy but not interesting. I tried web developement but it didn't interest me much. Although I was not interested in software engineering but even if I try to get a job as a software engineer, I won't get it easily because competitive programming was hard for me. You might understand that feeling of not good at anything or feeling of worthlessness. I really wanted to be good at something. At one fine day, I got introduced to ML for the first time during one of the tech meetup. Then at the beginning of my third year, I come to know of Sir Andrew Ng's Machine Learning course on coursera. I took the course, requirements were the school level maths and basic programming, so I satisfied the requirements. Andrew Ng taught that course at a level which made the ML look like an "easy" topic for me. As I was taking the course, I became sure that this is the thing I wanna be good at and I do not want to learn anything else. What I missed back then was that the Ng's course was just the _introductory_ ML course and is not the detailed one. The details presented in the course was just the "easy" tip of the great, "hard" iceberg. What I missed was that the course was not meant to provide mastery of the ML but just to inspire students to take more advanced courses. I thought this is what the ML is and I found it perfect because it appeared to be "easy" topic for me and also I found it very interesting. The idea of making the machine learn and solve problems by themselves was really interesting to me. That is how I decided to become good in ML and I do not want to learn anything else. And this was the beginning of the contradiction (more details later).


### Second course in ML
After some couple of days of me completing the Ng's ML course, I also got introduced to deep learning (DL) for the first time. It blew my mind and so fascinating. DL is the subset of ML. And until then, I was only aware of the classical (non-DL) ML algorithms like linear regression, logistic regression, SVM, decision trees etc. But after I came to know about DL, I was sure that I wanna be good at it. My whole interest was completely shifted to the idea of simulating human level intelligence which was only possible via complex framework like deep learning. That is why I decided to become an AI expert. And, I majorly focused on DL since then but not completely ignoring classical ML because they are the best candidate to solve some of the real world problems. 


I took Ng's DL course. Again the course was presented at the level which I could understand easily with my school level maths knowledge. I was under the impression that the course would explain every details of deep learning. But again I missed that it was just an introductory course, a "easy" tip of the "hard" iceberg.  I thought it is an easy topic, which I could understand in an easy way like I used to. 


My first ML course by Andrew Ng was a broad introduction to ML which mainly covered classical ML and the second course was specific to DL algorithms. After completing the courses, I came to know about the python libraries which makes implementation of ML pipeline easy. Scikit-learn is a general library for ML algorithm and Pytorch is specific for deep learning. 

### Feeling the contradictory situation
I first worked on toy projects. With default hyperparameters of these libraries, such projects become extremely easy, as you can achieve 100% accuracy easily. Then worked on complex, real problems on Kaggle. Kaggle competitions are based on real world problems set by real organisations who are tackling this problem. So, Kaggle competitions are great to test your ML knowledge. But first I participated in getting-started competitions (titanic survival prediction, house price prediction etc.) which are comparitively easy than the real comptetions. Getting-started competitions is difficult than the toy projects because it is hard to score in them but very much easier than the real kaggle competitions. I was struggling in getting-started competitions. I tried different getting-started competitions and was still struggling. When looking at the other competitors solution and kaggle discussions. I come to know that my solution is very basic and I can not think in a way, other competitors could think and come up with the solution.  I started to feel that my knowledge of ML is limited. My limitation of ML knowledge only gets more confirmed as I participated in real competitions.

That's when for the first time, I started to feel the _contradictory_ situation I was in. I was very much aware that I really struggle in "hard topics". If I know, that something is hard for me, I really won't spend my time on it (lazy brain huh!). But if I am really required to work on the hard topic (for example; a required course in college) then I would do it in an "easy way" (and end up having only narrow understanding). But I started learning ML out of pure interest in it and was really sure that I want to excel in it. (In retrospect, my interest in ML is like monotonically increasing function). I cannot just have narrow understanding of it nor I can leave it aside. I never felt such a contradictory situation before: a topic which is very much interesting and surety of mastering it but but it is next to impossible for me to master it because it is hard. 

### Ignorance: short term bliss
I do not wanted to confirm my seemingly contradictory situation, i.e., I do not wanted to confirm that ML is indeed a "hard topic" for me. Because if it gets confirmed then the contradictory situation is like [[[[[???]]]]]. 

        Learning the hard topic the easy way is like in a video game, you do not know how to swim, you have plunged yourself in the deep river to cross it and you are trying to flap erratically and initially it feels like you are moving ahead but in reality, you do not move much, loses the stamina and drown yourself. [make the stamina in the game infinite, ]

And then I _have to_ resolve the contradiction by either learning the ML by grueling "hard way" or find something else to become good at. I wanted to be perfect in something. The potential of AI is so great and also my interest in it, if I do something else then I will be incuring huge, huge opportunity cost. Because it is very unlikely that some other field has the same potential as AI and my interest in it will be as same as in AI. So, I never changed my goal of becoming an AI expert. It was engraved in me. Then the only way to resolve the contradiction was to learn ML properly by hard way, which is so grueling and next to impossible for me. 
          To feel the contradiction, feel like holding two ropes with the hand, each is getting pulled in opposite directions, tearing you apart, only way to stop getting teared up is to let go of one of the rope. So intead do not confirm the contradiction.

So I get away from resolving the contradiction by not confirming it. I would _consciously_ ignore those good resources of ML like standard ML books, top university lectures etc. Because such resources would only confirm the hardness of the ML by presenting the detailed knowledge of it. I was hoping that even if ML is a hard topic, what if I could learn it the easy way. It is just that I do not have an intuitive understanding (realizing that ML is just one form of my existing knowledge) of it yet. And I should explore more resources and continue learning. I was trying to deny the contradiction because I can not think of the impossibility of me learning ML.

(was in nice local minima of comfort, kaggle was good-false progress, good project, abundant learning resources, bag of trick)

### My goal of becoming an AI expert
The notion I had of AI expert was like this: One should have comprehensive understanding of ML (i.e. of _all_ the algorithms), which in turn gives these abilities - to improve existing algorithms or create new ones or the ability to use the algorithms to solve real problems i.e. working on projects.

An AI expert manifests his/her expertise through these abilities.

So, I cannot think of improving the current algorithms, creating new ones or cannot think of working on projects tackling real problems, until I have comprehensive understanding of ML.

### ML as a Tree



### Top Down Learning: A way I took in college to AI expertise
There were three factors which lead me to learn ML properly by working on the projects. 
First factor: I was comfortable in programming and already had narrow understanding of ML fundamentals from the Ng's courses. Which in turn made me very much comfortable with plethora of open source implementations of all the ML algorithms. When I ran some of the algorithms on my own and seeing the ML work balanced the uneasiness I was having for not understanding ML  comprehensively. Because I was _a_ step closer to one of the manifestation of an AI expert i.e. able to solve real problems with existing ML algorithms. With such state of my knowledge, atleast I had the confidence that given any problem solvable by ML, I could provide a solution with average performance. So that was a achievement for me in contrast to not able to solve the problem at all. Me being happy in atleast able to provide ML solution with average performance is like me being happy in atleast able to flap erratically in the deep water in order to reach the treasure. So, I am atleast a step closure to the treasure as opposed to standing on the bank and doing nothing. 

        Such understanding allow you to start working on projects and create solution with average performance. With libraries like scikit learn (for classic ML) and pytorch (for DL) and open source code, implementing the ML algorithms for any project is very much easy. By seeing the ML work on my own computer and able to get average performance, atleast makes one feel a step closer to AI expertise. Comfortness in playing around. 

Other two factors are these and their respective sections: Top Down learning is a thing and surety of going for a Job after the college.

#### Third course in ML
Soon after I started to feel the contradiction, I came across Sir Jeremy Howard's course on DL, which is popularly known as a "fast.ai course". Unlike most of the courses, it was based on top down approach of teaching. This is for the first time, I came to know about such approach. Learning anything can be approached in two ways: top down and bottom up. Bottom up approach is more popular and that's how we learned in the schools and colleges. Simply, first master the fundamenals and then learn increasingly complex topics which are based on the knowledge already acquired. For ex, to learn the words, we first learned the alphabets; to learn the physics, we first learned the mathematics; to learn the differentiation, we first learned subtraction, division and functions.

A bottom up approach to AI expertise is to learn in these sequence: ML maths fundamentals, ML fundamentals, all ML algorithms. Then you get the abilities mentioned above. 
    But that is what lead me to confirm the contradiction which I was avoiding.

Jeremy seems to be obsessed with creating state of the art ML solutions to solve real world problems. He is obsessed with making ML tangible, i.e. he is more of a applied researcher than a fundamental researcher (although he had created ULMfiT algorithm which became the basis of all modern LLMs). Fundamental ML research focuses on long term goal, having much broader and significant impact (more on this in later section). It improves the basis of all ML algorithms, hence have impact on many applications. While applied research focuses on how the current basic ML algorithms can be modified or upgraded to further improve the performance on a particular real problem. Every problem is unique and hence there is an opportunity to tweak a basic ML algorithm to best suit that particular problem. To summarise, fundamental research works on improving fundamental ML (tree analogy) which forms the basis of many ML algorithms, while applied research works on those aspects of the algorithm which is specific to a particular problem and do not tranfer to other application. Also, it focuses on how to solve a problem with that algorithm - which then require a work on dataset, implementation and deployment.   So, both kind of research are important. He has solved many real problems through ML by either founding a company or leading a team at other companies. He was also the best competitor on the Kaggle for 2 years, which made him the Kaggle grandmaster in the competition and notebooks track. That really proves how practical Jeremy is in terms of making ML useful - i.e. solving real problems with it.

Not only this, he is also very much obsessed with how ML should be taught. That is, he wants to make ML as accessible as possible. The standard resources - university courses, standard books etc. taught the theory of ML first and then practical (bottom up approach). Also, theory requires maths which is beyond school level maths. Making ML esoteric subject. I was also a victim of this. Many people who are not engineers know only school level maths. So, for them, learning ML directly through standard resources is not feasible or they have to first spend too much time learning the required maths before they can approach such resources. Also, they won't be willing to learn higher level maths ("ML maths fundamentals") because it would be more difficult for them as compared to engineers. Also, if someone knows the required maths, then they have to spend time on learning the theory first, which many would find useless until they see it working. So, overall standard resources are either not accessible for many or practical, he wanted to teach ML in a more accessible and practical manner.

From his teaching and work experience, he believes that one can _begin_ to learn the ML from code point of view (top down) as opposed to maths point of view (bottom up). Such a approach has a advantage that, it only requires programming and school level maths. Many people (engineers or non engineers) satisfies these requirements. Also, programming is easy to learn as opposed to ML maths fundamentals. Hence, making ML _accessible_. Learning ML from code point of view also helps you _aid your learning_ because you can either use the debugging feature which allows you to go deeper and deeper into the code, in a step by step manner, breaking the complex function into simpler easy to understand components. Or it helps you write helper code like various visualisation functions. Another advantage is that teaching starts with a state of the art (SOTA) algorithm which is capable of solving a real problem with best performance.  Right from the beginning, he would run and explain that practical SOTA algorithm. Then he asks you to implement that algorithm and run it for yourself (these algorithms are run on a smaller dataset, so that less compute is required and for faster iterations). Right from the beginning, you learn the ability to solve complex problems, writing and understanding narrowly a solution which is capable of giving the best performance. You feel a bit closer to one of the manifestation of the AI expertise, I mentioned above. So, such a approach is very much _practical_. 

Any algorithm of ML solves a particular problem - linear regression solves the real value prediction problem, classification algorithm solves the classification problem etc. So, each algorithm has two aspects - problem specific and problem independent. Problem specific aspect would be, e.g a DL network used in image classification, object detection and image segmentation are all different. Problem independent aspect would be the parts of the algorithms which are fundamental to ML, which can be applied to all the ML algorithms, e.g. optimizers like Adam, SGD, etc., learning rate schedulers like cyclicLR, transfer learning concept, regularization techniques like dropout, batch normalization and other tricks for training the model. 

So, just from the first lecture, he would pick a SOTA algorithm for a problem e.g. image classification. Since it is a SOTA algorithm, problem specific and independent aspect of the algorithm will be intricate. E.g. that algorithm would use ResNet50 architecture, Adam as a optimizer and OneCycleLR as a LR scheduler. He uses high level wrapper ("fastai" library) of Pytorch which abstracts away many details, makes the whole algorithm as a sequence of high level function calls, to avoid the initmidation. He would then peel off the abstraction one layer at a time. These high level functions are themselves made of functions, and those functions are themselves made of even lower level of functions. As, he goes deeper one level at a time, every underlying level gives more information about the algorithm. Every next deeper level exposes only those details which is easier to understand, given the understanding of the above levels than to go directly at the bottom and understand all the details at once. Also, one can understand why a certain intricate aspect of the algorithm like OneCycleLR is built in a way it is, by using the simpler version of it and finding the difference.  


 explanation of any subarea (e.g. computer vision) of ML starts with the state of the art practical algorithm within that subarea.


Jeremy's top down approach to teach ML, goes from code to ML fundamentals (maths part of the ML) while relying on the programming or computer all the way as a secondary brain for assistance in learning. In contrast, bottom up approach goes from ML fundamentals to code. So, you have to rely on your brain only (using pen and paper) to learn for the most part of this path and programming only comes at the end. Ofcourse, you can use the computer to aid your learning but other two advantages are not available i.e. accessibility and practicality. But bottom up approach makes you feel like a theoretical researcher right from the beginning. If that is your goal, then you would enjoy bottom up approach. But initially, many do not really know what they want to become, but just have a vague goal of becoming an "AI expert". And top down approach is the easiest way to atleast start manifesting the AI expertise.

Top Down Approach seemed to be perfect for me because of the advantages mentioned above. 
A top down approach assured me that one can indeed learn by first running the algorithm on the computer and seeing the ML work. And then try to figure out what is going under the hood (analyzing), that too via programming e.g. by writing helper code for various visualization. fast.ai course gave me the hope that by working on the code of the algorithm first (without mastering the ML first), you can indeed work through your way to AI expertise. 

Since as I mentioned, I was in a position to atleast able to start working on the ML code. Third course showed me a path to my goal which was perfect for me. First of all, I was satisfying all the requirements in order to tread that path - school level maths knowledge, ML fundamentals introductory knowledge and programming. Secondly, it ensured that you can indeed reach to treasure without knowing how to swim but for that, you must have _stamina_. 

That is, I can reach to the treasure without going through standard advanced ML resources, given that I have _enough patience_. As I mentione above, I was avoiding the good resources of ML such as core ML resources because they present the ML in a way, appearing to me as a hard topic. And making me feel the contradiction. But top down approach allows you to learn ML from the code point of view. Looking the ML from the code point of view never intimidated me, infact it was very much comfirting because I was good in programming and also to _analyse the given program does not require large attention span_ (you do not have to track every complex state of the algorithm in your mind, it is tracked in the computer memory, can offload your thinking process to computer). Every concept in ML is eventually meant to be run on the computer. So, you can access every concepts from the code point of view as opposed to the seemingly hard view of maths which standard ML resources uses to explain the ML. 
    So, top down approach to learning ML is - learn by running and analyzing the implementation of ML concepts. 

Learning ML from standard resources is a bottom up approach (analogy of building the ML knowledge tree). As these resources gave me the sense of hardness of ML, which means large attention span is necessary to learn.
In contrast to this, top down approach do not have special requirement of attention span, but instead it _requires patience_. So going from bottom up approach to top down approach of learning, requirement shifts from large attention span to having enough patience. At first, I didn't really know of this requirement of patience. On the first glance, top down approach looked way more lucrative to me because of my comfort in programming and using the programming to learn ML properly. So, I started the fast.ai course as my third ML course. 


  Instead of flapping eratically and quickly trying different people's swimming technique (i.e. quickly trying the open source code). You pick one and stay at it longer. You try to break that technique into basic actions and then master those basic actions first. Meaning, pick one open source implementation of any algorithm which suits for your project at hand. And, instead of running it quickly and seeing whether you are getting results, you try to zoom in and try to understand what each intructions in the code is doing. That is, be conscious of each operation in the code. For example; you running a linear regression model and find it performing well on train set but not on test set. Top down approach asks you to figure out what is going on. You analyse the results and find out that the weights have abnormally large magnitude, which makes the model fitting the noise in the train set. This analysis gives you the idea of fixing it by penalizing the model with large weights. You realize, that is what the regularization in ML means - trying to find the "regular" weights. So, you learned the concept of regularization through top down approach.

  That's what Jeremy Howard did in the course. From the first lecture itself, he started with the notebook. First ran the code. And then explaining each functions in it. And then explained each intructions in each of those functions. 

  His idea was to first start with the basic solution. Basic solution explains the ML fundamentals clearly. Then figure out its weakness and then iteratively improve it. Everything is programming first, you look at the ML from the lens of the programming and not mathematics. So, from day one you are manifesting your AI expertise. Starting with the average solution then improving it. Every bells and whistles added to the basic solution is first analysed with the code in terms of how well they are performing. 


> connecting with job
If my hope is false (which in retrospect, was entirely false), then I do have an opportunity to learn properly during the job. And to get the job, I should have enough understanding (if not comprehensive) of ML so that I can at least get an entry level job (sort of trainee) and get inside a company. 

#### Surety of doing a Job and its _seeming_ advantage
I was sure that I do not want to go for formal higher education - masters or PhD. Because then I have to only work on hard topics in very much limited time (limited time atleast for me). Too much stress for my baby brain. Longer attention span is must for higher formal education. Without it, you won't be able to work or study efficiently and hence complete the syllabus with comprehensive understanding before the exams. What is the point of becoming the "master" or a "doctor" if the only way is to rote the syllabus. 
Also the notion of taking gap years after the college and do self study was not popular. So, I didn't even thought about it. Only popular options for post undergraduate were: formal higher study or job. So, only option I was very much sure about was job or more precisely a job in ML.  There were some factors about why I was sure of the job option: first of all, it was the only option. Second, but more important was the excitement to work crazily for a ambitious company and bring some disruptive changes. 

Excitement to work crazily because first I will be working in the field of my interest (ML). Secondly, there is always an momentum in an ambitious company toward the growth. They create the momentum by having a great product infused with state of the art technology like ML, a motivated team and productive working environment (office, table, chairs, AC). So I thought if this momentum gets aligned with my interest in growing towards AI expertise, I will be forced (positively) to push my boundary of knowledge in ML ("hard way"), which in turn not only help me grow towards the AI expertise but also help the company grow. I thought, the force to push my boundary might ignite my own engine and I will be all my own on the hard way (i.e. working on all days, full time, just pushing the boundaries of me and the company). Such a win-win situation makes me feel like to work crazily.
      If I work in such company as a ML guy, I will be also _required_ to create the momentum toward the growth through ML. So, working crazily not only help the company to grow but it will also help me 
The money factor of chosing the job option was not in my conscious mind. I would have worked in a low paying company, if there is a guarantee that I would move toward becoming an AI expert.

So, for job, I _needed_ to make good enough projects so that I can get entry level job.

### Focus on Project
These three factors made me focus on the implementation side of the ML. 

I started the fast.ai course but I really realized the patience factor which was required for the top down approach. It takes time to start from the leaves and move towards the roots. But if you continue, not only you will have good understanding but also some good projects.

I didn't had enough time to continue that path. Soon my placement would begin, so there was a pressure for me to create "good" projects, so that I can put them on my resume. And projects was the only way to show my ML knowledge and to expect a ML role. I decided to work on projects.

Characteristics of good ML project:
As I was more comfortable with top down approach as opposed to bottom up approach, I could have only thought of creating an applied research project.
I first learned what is even mean by a good ML project. Good applied ML projects are the original solutions that solve a real problem with good performance. Real problem means, it should use real dataset and not the toy ones. Original means you have created the solution by yourself and not copied it. Getting a good performance on a real problem means your solution is either closer or beaten the SOTA for that real problem or dataset. 

With the narrow understanding of ML, it was not easy to create such a project in an elegant manner or with first priciple thinking. But Kaggle and fast.ai course atleast enabled to me to create a project closer to a good project. That is, I could create a project which looks intricate and could achieve above average performance on a real problem. Achieving above average performance is not easy with the narrow understanding, but I could do it because of my third course. My little exposure to the fastai course instilled in me the skills of a _hacky research_. I would have become a good researcher, if I would have completed the course.  With a hacky research, your project looks intricate and can achieve above average performance. And kaggle was a best platform which made the hacky research simpler. Intricacy may not be necessary to achieve such a performance. But I knew achieving good performance is next to impossible for me, so I thought my project should atleast look intricate and I should anyhow able to superficially explain it. Intricacy was important for me because SOTA algorithms are generally intricate, hence it signals that I can create SOTA like algorithms. 

fast.ai made me aware about how a typical SOTA algorithms look like. It taught me a way to atleast have the enough understanding of the aspects of these intricate algorithms, so that I can try them on any problem. 

My hacky research skill was like a car and kaggle acted like a perfect abundant fuel. 
 


    So, how do I get comprehensive understanding (or at least enough understanding so that I can get an entry level job) of seemingly hard ML through the "easy way"? When I was taking the Ng's courses, I thought, it just clicked to me and I have a good understanding. But later found it, it was only a narrow understanding. ????????Since I really wanted to cross the river without knowing how to swim and get to the treasure. I was hoping that I would find a right bodily movement by just flaping erratically. Meaning I was hoping to find a right resource (rightness depending on the level of knowledge I have already, which excludes the detail revealing resources) that would just click to me. I would keep learning from here and there. Of which, a resource where I spent my most time was Kaggle. 

#### Kaggle was perfect for me
Kaggle was a perfect platform for me because it was all in one. Freely providing - learning materials; compute resources like cloud hosted notebooks powered by GPUs; real world problems coming from real organisation tacking those problems; rewards to improve your learning in various ways (gamified enviroment) like earn a title when you share your knowledge or code, or when you improve your score on the leaderboard. For me the main rewarding experience from the leaderboard was how many ranks I jumped after improving the model. Hopping up a bit on the leaderboard was enough a motivation for me to keep improving the score. It was perfect because I could learn and also could check my learning by applying on the real problems. In a sense, I was learning and working on the good projects parallely. Learning materials were in the form of discussions and open notebooks. And kaggle was perfect for me because these learning material presented the knowledge just at the level where I could try to understand them intuitively, i.e., only based on my existing knowledge.

#### Special Interest In Computer Vision
Some context. During my college days, I mostly focused in Computer Vision (CV, a subfield of AI, I find it fascinating): giving the computer an ability to perceive the world, infer and take actions accordingly. Although a single system achieving an overall goal of CV is far away but the subproblems of CV like image classification, segmentation etc. have their own significance and applications. 


#### Hacky Research
Hacky research is nothing but _hit and trial guided by narrow understanding_. To solve a problem with ML, one has to take decisions at each of these steps of ML workflow: what data to use and how to collect it (if its already not available), how to preprocess it - which in turn requires many decisions, how to split the data into train, val and test, what model, loss function, optimizer, evaluation metric to use, How to make overall pipeline efficient and many more decisions while deploying. At each step, there are many options of choose from. E.g. in image clasification, during preprocessing, which image size to use to resize the images in dataset. 


With comprehensive understanding of ML, you do not require hit and trial. Because with comprehensive understanding of ML, you have the ability to analyze the problem or data from first principles. With this ability, you can break the complex problem into simpler problems until you fully understand it. With comprehensive understanding, you know what ML algorithms look for in the data. So, you can look the problem from the ML algorithms point of view and try to figure it out how the model will process each feature of each sample. That is what breaking into simpler problems means, in contrast to looking at the dataset as a whole. Example in the following para. Once you understood in terms of simpler problems, you will know close to precisely what options to choose for each decision to achieve good performance.  

Without no understanding of ML, there is no guidance in the search and hit and trial is like trying to hit the bull's eye in pitch dark. Not knowing a bit, in which direction to try. 



With narrow understanding, you cannot analyze the problem from first principles. But I had access to 

notebooks, discussions
as I mentioned above, I could  

Atleast, you can easily find the options which will give you above average performance, after some hit and trial. But getting better performance is exponentially difficult. The search space is so big, that you cannot hit and try every possible combinations of options. And for DL algorithms, it is specially true because running them on huge data is very very slow on freely available GPUs. That is, with narrow understanding 

 can figure out the probable options out of many options for each decision. With these set of probable options, you can get the above average performance, .  But getting better performance becomes exponentially difficult.,  still, 




Now, I just needed a real dataset, Kaggle was perfect.
But fast.ai course made me more comfortable in understanding existing implementations of some intricate ML algorithms. Made me more comfortable in reading other people's solution on kaggle for competitions based on real problems.


#### Two Kaggle Competitions
Two Kaggle competitions which I have participated seriously were leaf disease detection (image classification) and glomeruli detector in kidney images (image segmentation). Both competitions tackled the social problems.

Since I was interested in "AI", my main projects were based on DL. 


#### Copy - slipping to bad local minima



#### Easy way only gave enough understanding to get the job

[SHIFT IT TO APPROPRIATE SECTION]
Time spent on any resource through an easy way depends on attention span, e.g, for smaller attention span, you would spend comparitively less time because you will get bored much early (more explanation in a later section). But whatever time I spend through an easy way, I will hope for the click moment. While through hard way, time spent is actually the time required to comprehensively understand that topic. If the topic is easy then even the small attention span would be enough to understand it properly (having the click moment). If the topic is hard then I will surely get bored (frustrated) before I could really understand it properly. 

When I learn from any ML resource, I would not understand it completely but atleast I had enough understanding, which I used it to improve the project (in term of the metrics like accuracy). Not able to understand properly gave me the sense of hardness of ML but I ignored it and rather focused on how I could improve the project. Ignored the hardness of ML because it is a special case of ML. If there was some other topic, which I didn't understood in my first try (or in rare cases, I would give a second try), I would declare it as a hard topic and would never touch it. Because for me, narrow understanding of any topic is like half cooked food and would make me feel unease. What is the point of trying again, if I know it is next to impossible for me to cook it properly.

But for ML, I did multiple iterations of learning in an easy way from different resources (blogs, videos, kaggle discussions and notebooks) and applying it to my projects. In every iteration, I had narrow but different understanding (i.e. everytime I will realize a new form of my existing knowledge which is closer to ML). Having a narrow understanding didn't really satisfied me. The satisfaction of learning this way came from applying the learning in the projects. Sometimes it will improve and sometimes it won't. But whenever it improved, it gave me the sense of satisfaction (but ???in retrospect, it gave a false hope or false motivation which won't let me to the goal). 

   

Whatever understanding of ML I had, it atleast gave me enough confidence to solve these problems and allowed me to achieve _above average performance_ (average in terms of other people's solution performance). I also had the confidence to be able to keep improving the performance, if I spend more time on it. So, there was no uneasiness of narrow understanding because I was able to use that limited understanding to make the ML work. That is what mattered to me at that time, to be able to make and see ML work which is not only interesting per se but it is also solving the real social problems. _In the college days, these 2 wins were enough to overcome the loss of easiness which comes from having comprehensive understanding_. Because with this achievement, I knew I could get atleast a ML job in a startup and would hope to learn there by working crazily (as I explained above).

Although I had the confidence to create a solution with really good performance, the only _problem_ was that I was not certain how long it will take to achieve it. I was certainly able to achieve above average performance in short period of time. But the further improvements were exponentially difficult for me but I had atleast the hope that I could improve it, if I have enough time. I was able to think in this manner only because of my deep interest in ML. That interest drove me to continue working even if there was uncertainty of time required. 

#### Guided Hit and trial


Hit and trial 



Answer to all these decision creates a ML algorithm tailored to solve a particular problem.

[uneasiness of narrow understanding was balanced by improving the projects. That is atleast able to solve problems by ML although not in a structured manner and elegantly, or from first principles]


### Was slipping to Bad Local Minima
- My learning of ML (through easy way) was guided by the requirements of the projects. Why projects? first as I mentioned above I won't spend much time on learning from the resources and to balance the uneasiness from narrow understanding, making and seeing the ML work was joyful. So narrow understanding and working on project goes hand in hand and are inseparable. Secondly, I needed to create projects to get the job.
-  was missing the bigger picture
- my learning was mainly coming from project based learning. But in project based learning, there is fair chance that one can take short cuts in learning by making merely "good looking" projects. Good looking projects can be made by guided hit and trial. But it is a shortcut because not only you do not solve that project elegantly, efficiently, with good performance but you do not have much advantage while solving other projects because every problem is different (scalability issue). And since research is highly evolving, even solving the same problem again is difficult as if you are solving for the first time . [visualization of traversing a hugeee tree of problems and of AI algorithms, starting from leaves and getting stuck there vs starting from root and moving fastly. Leaves are problems, trunk is ML maths fundamentals, when you solve problems you learn about associated algorithms that too narrowly]
- I focused on project based learning because of two reasons, 

- I feel the pain of this scalability issue but the other way was the hard way. Project based learning was the best way I could have utilised the narrow understanding.

- Narrow understanding 






  I would really wont enjoy the half baked food in every iteration of cooking a. But for ML, even If I was having half cooked food, I was able to use it in the project, just to feel that food is being cooking better with time. But it was false, I would never have comprehensive understanding. 

  means  spending less time on learning before I get frust, hoping a click moment while spend your time on learning. Your learning is complete. It's time to check your learning. I will apply whatever I have learned in my project and look for the improvement. 


would look like quick learning from any given resource, hoping for that click moment and then checking the understanding by appl
manifests as either project based learning or solving a problem by hit and trial.
easy learning manifestation, hit and trial.

This will repeat, learn and apply, project based learning, hit and trial. Only having narrower but different understanding of ML in each iteration. collecting bag of tricks.



  The goal of CV is difficult and still haven't achieved, but it can be broken into many subproblems like image classification, object detection, image segmention, depth estimation and many more. These subproblems are easy to tackle individually and have really good algorithms which perform well (although they fail terribly whenever they fail and many other fundamental issues which I will mention in a later section). But making a single CV system which atleast gets closer to human vision in performance and generality requires a fundmental change in the way AI is . 

  [vicious circle]
  I have problem of wanting to be good, but i can only become good while at a good job (becuse it is easy to become good while at job). But to get a good job, you have to have some good projects. Good projects can only be made if you are good. To break the circle, I have to compromise and find a nice local minima - a good job acc. to my whatever good skill set i can achieve.



  As I was kaggling, I come to know one more weakness of me - problem solving - which I knew (from my competitive coding experience) I have but until then I really didn't think of it from the ML perspective. Problem in ML is described in the form of data and the task you want the machine to perform on it. For example, if task is classification then first thought comes to mind is of classification algorithms. So, one can think of the possible solution instantly, if they even basic knowledge of ML (like I had). But that instant solution only gives you average or below average results. It means it does not solve the problem very well, better solution is possible. When the problem is described in the terms of data and task, that is just the high-level, not fully detailed problem description (not mentioning the root problems). And your instant solution was just based on such description. And it does not solves the root problem very well because you really do not know the real problem from the high level description. Finding the root/real problem from high level problem requires the "hard way". So either you will 
  I could not solve the problem from the first principles, in a structured manner or elegantly. I would describe the problem solving from first principles as breaking the complex problem into most basic/root problems, then thinking of solution to fix those basic problems, in order to fix the bigger problems. Once you identified those root problems, then you thinking of solution is easy but breaking the complex problem into root problems requires "hard way" (concentration).  


- I didn't looked at the fundamental books like PRML, ESL, ProbML etc. They were like too detailed for me to look at them. I was trying to learn from here and there, in easy ways, hoping that they provide strong intuitive understanding of the ML. From easy sources, hoping that ML still look an easy topic for me but I understand it better. But for ML, comprehensive understanding will only come to me in hard way and I really didn't realised this back then. [Bag of tricks, looking at the iceberg from different angle, having more and more but only narrower understanding, feels like progress but either it is just a false progress]


- I am not able to understand ML properly because of maths.  ML is nothing but the maths, data, program and computer. If you have these four components then ML is very much easy. With maths you create or understand an algorithm, with programming language you implement the algorithm, with data and computer, you make the alogrithm work. Out of three steps, only first step is hard for me, rest of the two are easy. ML is a hard topic because it is just an application of ML maths fundamentals. And ML maths fundamentals is hard, if I master it then ML becomes easy. 

- First of all, I was not aware that ML is a hard topic and can only be learned via hard way. And also I was not really aware of this breakdown that ML is only seemingly hard because I do not know maths.  


        


        
        I wasn't fully aware of the size and hardness of the iceberg until the mid of this year, when I came across the Sir Kevin Murphy's comprehensive treatment of ML with his two part book.        
        
        I learned ML from online basic courses. From these basic courses, knowledge comes to you in three forms - intuitive explanation, maths and code. Intuitive explanation gives ones a feel that he has understood it very well, but infact you only understand it at surface-level. In reality, intuitive explanation is just the introduction of the algorithms, it just stimulates you to dig dipper. Mathematical part is the rigorous, detailed explanation of the algorithm. You understand the algorithm inside out. And more importantly, maths tell you _how_ that algorithm was created from the first principles. And it what scenarios, this algorithm is supposed to work or where it will fail, etc. (and it allows you to use that algorithm effectively, it gives you "comprehensive understanding"). Code is just the implementation of the algorithm and it makes the algorithm live. It enables you to play with the algorithm, verify your understanding from the maths part and solve real world problems. If you know the maths part and basic programming, then you can easily understand the code part. But vice versa is not true. To _easily_ understand the maths part, you should know ML maths fundamentals or I will simply call it "maths fundamentals". Maths fundamentals are beyond the school level maths. If you are only good at school level maths then to understand the maths part is hard and requires a lot of patience. And to become good at maths fundamentals is also hard and requires a lot of patience (But maths fundamentals mastery is so rewarding, I explain its worth in the later sections).  At least, hard for me because I could not concentrate for long enough. At that time??, I would do anything to avoid the hard parts.

        So I only relied on intuitive explanation and the code part. I will call it the "code-way" of learning ML as opposed to "maths-way". I was really under the impression that I could understand the ML via code way because I knew programming. And school level maths allowed me to have an intuitive (surface-level) understanding. I was under this impression because I could not imagine myself learning maths fundamentals with full concentration (I can imagine now because I now know the worth of maths fundamentals). So I cannot see the other way apart from the code-way to become good at ML. Also, I didn't even know what I even meant by good at ml or AI expert. Only after working on some good projects, I came to know that what kind of AI expert I wanna become and math-way is the only way for me to achieve that.

        It turns out that code-way is limited in the sense what you can learn from it. Limited because as I mentioned earlier, code is just the implementation of the algorithm, it won't tell you how that algorithm got created via [first principles thinking](https://jamesclear.com/first-principles). You won't be able to learn how the researcher's (creator's) mind work, so it is hard to become a researcher or creator. With code-way, what you have at your disposal is the already built product - an algorithm. So whatever code-way has to offer for you to learn is actually whatever you can learn from the end product. Learning from the end product - algorithm, guided by your intuitive understanding of it, will only allow you to learn "how and where you can use that algorithm effectively to solve the real problems" (tool-based understanding). That is good outcome but learning it is not easy. It requires work. It requires you to answer these questions - "where this algorithm fails?", "where it works?", "Where it will be a good compromise?". These questions can be answered by running the algorithm differently multiple times and analysing the results. Real pain is that you have to do that for each algorithm to know how each one can be used effectively (I discuss about it more in later sections and how math way is a very scalable way and allows you to have comprehensive understanding as opposed to limited understanding of the code way). It requires patience to learn about all the algorithms in the code-way. So, I didn't even learn via code way. With the code-way, one what gets is the "tool based understanding" of the algorithms. One knows which tool (algorithm) to solve a particular real problem effectively. But without code-way learning of the algorithms, one still can use the algorithms as tools by relying on the intuitive understanding. But since intuitive understanding from the basic courses is just at surface level, you won't have a good sense which algorithms are effective for the given problem, i.e. you will only have "limited tool based understanding"

        For the classical ML algorithms, what I had is just the limited tool based understanding. For deep learning algorithms, my learning journey was just the same. For deep learning algorithms, learning via code-way is more difficult than for the classical algorithms. Because to run, it requires a lot of resources and time, so forget about running it multiple times. But since I found deep learning interesting (as I mentioned in the first para), motivation was just enough to drive me to have a better intuitive understanding. Which lead me to explore other forms of knowledge which majorly are research papers and blogs. But this form of knowledge gave just the better intuitive understanding and nothing more because as always I skipped the math in them. 



        That's about my first learning session of ML (including classical and DL). This is what I did in sequence - basic personal projects, job, second learning session, advanced personal projects, blogging. I will talk about each of them in subsequent sections. Also let me summarise the levels of understanding of algorithms I mentioned - intuitive, partial tool-based, tool-based and comprehensive. 


        I did personal projects with limited tool based understanding. With such understanding, one can only solve the problems by hit and trial method guided by (surface-level) intuition. You have a dataset and 

- How I competed in Kaggle Competitions or worked on my projects?
- 


When I started learning ML, I could see there is some maths involved and some code is involved. Code is just the implementation of the maths. Understanding maths is hard because it requires pen, paper and hard work. At that time I was lazy, so I would do anything to avoid the hard parts (more details on this in later sections). So to learn ML, I was naturally inclined towards the code based learning and only relying on whatever mathematics I had learned in my school and first year of undergraduate. It turns out, one don't even need to code because code is already available either in the form of libraries or from github. When one starts learning ML, they start with classical ML algorithms, for which there is a comprehensive library - "Scikit Learn" already exists. This library is off the shelf, clean, nicely packaged implementation of maths. It implements the maths inside the functions. And just exposes the high level function signatures to the users, hiding all the details of maths implementation. So with scikit learn, even the code looked so easy. Hard part of implementing the maths for yourself can also be avoided. But all this inclination towards the easy part is just the trick of my lazy brain to avoid the hard part. First avoiding the maths altogether. Then avoiding the implementation of it because implementation is already available. Trick goes on. I could have digged deeper into the scikit learn library to learn from the underlying details or could have just looked at the from-the-scratch implementation (exposes all the details) of ML algorithms available on github. But I didn't. 

So, I solely used scikit learn for my projects involving classical ML algorithms. Completely avoiding all the difficult parts. I wanted to learn the algorithms, how they are working internally, instead I learned how to just use them as tools (more on this in later sections).  And I was not even good at using them as tools to solve a particular problem. Because there is one more hard part I completely avoided which is actually problem dependent. Which is understanding the given data and find useful features (feature engineering). It requires analysing the data and lots of plots. 
                        When I worked on some standard ML projects (like house price prediction, titanic binary classification, loan default prediction and others) via scikit learn library, it really felt so mechanical. You start with the data, data preprocessing, feature engineering, split the data, model selection, train and evalutate. For each step, there  That's the standard procedure I learnt.  
I continue the same behaviour in my deep learning journey of avoiding the hard part and learning via code. But with libraries like pytorch which exposes more details but not all of it.

_Even learning from from-the-scratch implementations is limited and inefficient._ 
Even if the code exposes every mathematical details or one digs up the libraries, learning via code and avoiding the mathematical details is limited and inefficient. _Limited because you won't learn some details of the algorithm. In the code, only the final equations is implemented, for example loss function and weight update equation. And code contains no detail about how they are derived or is there other subtle modifications._
              _Inefficient because (I talk about it in later sections) whatever details which are present in the code are not present nicely, in explicit manner as in maths. And details will only come to light, after you run the code multiple times and create different plots. And it won't be reveal to you in _explicit_ manner (as in maths way) but rather you will have to _infer_ it from the results of the code run. So, indeed it is not easy path (for example you trying to find whether BatchNorm is really effective or not), I was looking for._  For example (also MCMC), if one wants to find the behavior of L1 and L2 regularization, then he runs the code multiple times for each of the setting. And find the behavior by analyzing the weights. If one wants to find why L1 and L2 have this behavior then he has to find what plot to create, to extract more details out of the code. But in maths way, everything is derived from a known starting point in a step by step manner, all the details get revealed to you in 
              you will only know them after you run the code  take some of your time before it comes to light.
  
This limited knowledge is not enough to improve existing algorithms or create new algorithms. But learning via maths, one understands the algorithms "rigoursly", every detail of it, it empowers you to improve existing algorithms or create new ones. 

## About my job as ML Engineer
That's how I self-learned ML and DL during my undergraduate, relying on high level code and whatever details I learned from the course videos. I was sure that formal education is hard for me so I never thought going for masters or PhD. So to become an AI expert, the only path was through job. And I was totally ok with that, thought that by solving real world problems alongside the experts and _under the pressure_, I will be _forced_ to learn ML properly (in a maths way) and hence grow toward becoming an AI expert. At that time, in 2020-2021, ML roles for freshers were rare as compared to today. Among the broad two categories of ML roles - ML engineering (MLE) and ML researcher, ML researcher role for freshers was more rarer than MLE, as one could tell. But at that time, I just wanted to work on ML, I didn't care whether it is an engineering or research role. Given my level of knowledge, I was lucky enough to get a job in a startup and worked as an ML engineer for 2 years. Although on paper, the role was of "ML Engineer" but in retrospect, I could tell that most of the time I was working like a researcher. Because the problems I worked were all open problems and no available solutions satisfied the team manager. For some problems we had a benchmark data and for others, we created the benchmark data. Almost all the time, we were constantly trying to improve on the benchmark data. You could tell already that, I was not improving on the benchmark through a math-way, in principled manner but rather in a code-way, doing hit and trial. I was really a hacky researcher and hence inefficient (would take long time to do little or no improvement). I would read relevant papers, either "try" the open-source implementations (if exists) or my implmentation of it (depending on my limited math knowledge). Or for some problems, I tried to create my own algorithm or improve the exisitng ones, in the cases where I could not make the existing algorithms work or just out of desperation. But all these were done in a code-way (trying things via only code and see if they are working or not, hit and trial). Because either I would not try to understand the mathematical details in the papers or even if I try, I won't understand much. On or off the job, These are the exciting projects I worked on: Causal Discovery in Time Series, Root Cause Analysis (RCA), Legal Document Question-Answering (in pre LLM era), Biospeckle Activity Recognition, Kaggle competitions and others. 

Two important things I learned from my 2 years of job experience. First is that better understanding of exactly what kind of AI expert, I wanna become. I would categorize AI experts in two broad categories - Engineers and Researchers. Researchers are good at better understanding and improving the exisiting algorithms or in creating new algorithms. And engineers are good at understanding the exisitng algorithms and making them available by creating complex scalable data, train and inference systems. So AI engineers work at an intersection of AI and software engineering. Without any of them, AI is not useful, so both are equally important. After my 2 years of job, I come to know that the work I love is pushing the boundary of AI, making the current narrow AI models more human-like, more general, robust and efficient learner. That is the kind of work I wanna do full time and this is exactly what a researcher does - pushes the boundaries.

Second thing I learned is that job is also not a path for me to become an AI expert because job for me is a vicious circle. 
I thought during my job, I will be forced to learn ML properly (in a maths way). And indeed, I had to force myself to learn ML properly because without it I can not solve the problem and deliver the results. But this is a vicious circle - "learn ML properly" and "faster result delivery". Second problem causes the first one, but when you try to solve first one (which is good for me), it makes the second problem worse. Because learning ML properly takes time but you also have to deliver result. So to break the vicious circle, I had to become a hacky researcher (explained above). In hacky research, you try different solutions in a hit and trial manner guided by existing ML knowledge. And hacky research do not provide signficantly good results but only little to no improvement. This little improvements keeps you going in the job but you won't be able to learn ML properly. Even if you learn by trying different solutions, you will learn very slowly but not properly. 


## Importance Of Maths: Part 1
After stating my learning and job experience, let me now tell you the importance of mathematics to learn or create AI. Ignorance of which lead me to fail in learning and not so good work at my job.

Mathematics is the language by which we (humans) communicate "How to be intelligent?" to the computer. Intelligence in simpler terms is two fold: First, it should be able to self-learn, i.e., to improve or increase its knowledge with time by interacting with the environment. Second, to take appropriate (acc. to human standards) actions based on the input (for example, images, text etc.), current knowledge and the context of the input. Computer can only represents the data (or input) and knowledge in the form of numbers and can only manipulate the numbers, so that's where the maths equations come into the picture. By the help of human provided maths equations, it can become intelligent i.e. can self-learn and can take appropriate actions. Self-learning will involve manipulating numbers representing knowledge, such that to improve or increase its knowledge. And taking appropriate actions also involve number manipulations. 

Without the strong mathematical knowledge, how one can communicate the computer ahout how to manipulate numbers to simulate better intelligence. Or how one would be able to understand that how exactly the researchers has made the existing AI models intelligent. Although, without strong maths fundamentals, one won't be able to have computer better intelligence or understand their existing intelligence, unless he has a lot of patience and uses his basic maths knowledge to create or understand the intelligence.

The level of mathematics (I will call it "ML maths fundamentals" or simply "maths fundamentals") which is used to communicate intelligence to the computer is beyond the "school level maths". I am only good at school level maths and all this time, I was ignoring the ML maths fundamentals. But with school level maths, it is very hard hence extremely time-consuming to understand the language to communicate intelligence. I surely gave up whenever I tried to understand this language because of the laziness. 

      I was ignoring it because I was simply lazy to learn it (later I would elaborate why learning maths would make one lazy). And I would try to make my way through my job by just assuming about how already build AI models are working. So only had a half-baked knowledge of existing AI algorithms and surely won't be able to create new AI algorithms or improve the existing ones. Hence, I could not afford to be an AI researcher, the role I aspire for. Because it mainly deals with the questions "How existing AI models can be improved in terms of intelligence?" and not "How existing AI models can be implemented to use them at scale?". 

## Contradiction

Here is the contradiction: **I find the idea of Artificial Intelligence so interesting and want to master it but I am also not trying to master the required mathematics**. I like AI so much that I never think of switching the field or job role. So to resolve this contradiction, I need to make the mathematics part of the contradiction to align with my interest. 

From time to time, I tried to resolve this contradiction by learning mathematics but couldn't learn it properly. After every iteration of me studying mathematics, I would end up with little more knowledge of mathematics but it would not be enough. 


## Resolving The Contradiction
Events (ChatGPT becoming popular) happened such that now I am working full time to resolve the contradiction. My first attempt was to again iterate over the mathematics but this time, in a systematic manner. I went through Linear Algebra, Probability, Statistics and Calculus. After spending the longest time on mathematics, I tried to test my knowledge by trying to understand the current research.

After looking at the open problems, I came to know the limitations of deep learning like robustness, continual learning, high sample complexity, interpretability etc. Since computer vision is my favourite among other subfields of AI, so looking at the potential solutions accordingly, lead me to compositionality and causality. And research of Francesco Locatello, Yoshua Bengio, Bernhard Scholkopf, Geoffrey Hinton and others. Surely I am not good enough _yet_ to properly understand their ongoing research. So, I thought to trace back their research. And find the sequence of key developements which will lead me to the ongoing research. This lead me to Boltzmann Machine (BM)(Hinton et al, 1985) and Hopfield Network (HN)(Hopfield, 1982). BM built upon the ideas of HN and both uses the ideas of Statistical Mechanics. The goal of BM was so ambitious as it tried to solve a broader problem of computer vision - classification, completing the partial image, imagination and understanding the content of the image. In theory, it could do all that but it was slow and not scalable. In 2006, Hinton showed that RBM (a form of BM - Restricted BM) can be used to train "deep" neural network in a better way (before this paper, training deep network was very difficult) and it gave new hope to the deep learning research community. 

The idea of tracing back and finding an older but key paper is good because when understood, it gives better understanding of multiple key developements based on it. Also, since it would be an older paper, so I thought it won't be too hard to understand (less high level concepts and closer to fundamentals). I traced back upto BM. I later came to know that BM is a good starting point, as it is also an important developement in terms of deep learning research. Because of this paper, there was a revival of deep learning in mid 2000's. Fun Fact: In [September 2024](https://github.com/joshi98kishan/deciphering-hopfield-network/blob/1475cb7d8a679f91cf78e58c8570166700383dff/README.md){:target="_blank"}, I decided to study Hopfield Network and Boltzmann Machine  and on [8th october, 2024](https://www.nobelprize.org/prizes/physics/2024/prize-announcement/){:target="_blank"}, Noble Prize in Physics was announced and it was given to the creators of these two models.

To understand BM, I had to understand HN and some simple concepts of Statistical Mechanics (field of Physics). This is for the first time, I took a paper seriously and tried to understand _every_ part of it. Full understanding comes when you see it working, so I implemented them too. This lead me to write three blogs on BM, HN and Statistical Mechanics (Stat Mech). Because as we know, in order to explain something _properly_, one should better know it. So explaining by writing helped my understanding. There were multiple experiments in both the paper (HN and BM) to show the capabilities of each technique. While there were no exact details about how those experiments are implemented, I implemented them based on my understanding. For HN experiments, there was a very slight difference between the results I obtained vs results mentioned in the paper. For BM, I was able to perfectly achieve all (total three) the experiment's results except for the third one. I was inefficient in terms of finding the right set of hyperparameters of the algorithm, it took me a lot of time and I could not spare more time for the third experiment.


[Move these links and corresponding explanations to seperate post]
The blogs are almost in their prefinal versions. Here is the notion link of each blog. And code for HN and BM:
- [Blog-Thermodynamics-Statistical Mechanics-Ising Model-AI](https://www.notion.so/Blog-Thermodynamics-Statistical-Mechanics-Ising-Model-AI-2bfa87ef96fd4cbfba09b493a6250cb2?source=copy_link)
- [Blog - Hopfield Network](https://www.notion.so/Blog-Hopfield-Network-1202f5954fbf80ac9f06fa5929c14493?source=copy_link) ([Code](https://github.com/joshi98kishan/deciphering-hopfield-network))
- [Blog - Boltzmann Machine](https://www.notion.so/Blog-Boltzmann-Machine-1202f5954fbf808ea6d8de1cfee7b30b?source=copy_link) ([Code](https://github.com/joshi98kishan/boltzmann-machine))

All these blogs are little messy and are 90% complete. You would find the grey colored text redundant. Unique selling point of these blogs is that I tried to make them self-contained. And there is complete, clean derivations of required equations in HN and BM.
I haven't completed them because it took me a lot of time reproducing the results. Since the writing part and experiments were almost done (except the last one), and I was not sure how much time the last experiment would take, so I felt like moving to the next key developement - RBM. 

While working on RBM, it occurred to me that I am very inefficient in terms of understanding the paper (taking long time to properly understand them). So, it would take me longer time to reach to current research. And that's my latest, strongest motivation for learning the maths. Why strongest? Because I had just spent 2 full months learning the maths and still I am inefficient to understand the papers.

 
After spending some time thinking about it. I just started to understand how should I understand the maths. I should understand the maths not just from the level of equations but I have to go more deeper until I break the maths concept into basic operations of plus, minus, multiply and divide. I.e. from first principles.

Learning maths is difficult because there so many layers of abstractions. Until you  






## Importance Of Maths: Part 2
Here I kind of compare what is the difference between learning with weak vs strong ML maths fundamentals. About spending time on each method and having less understanding vs spending time on one thing and having comprehensive understanding of all the methods.
With weak maths fundamentals, you can not even understand the method comprehensively, so to know atleast tool based knowledge (how to use it), I would rely on the paper text (i.e. intuitive understanding): how they propose to use it, where it can be used and its disadvantages, 

- remove the high level maths methods
- replace "methods" with algorithms
- explanation of what is "maths fundamentals" is already provided above
- replace "tool based understanding" with intuitive understanding.
- My MCMC example below is just one example of "intuitive understanding"
- With mastering of the maths fundamentals, comes the ability to understand hard ml topics from the first principles, which resultls in comprehensive understanding. Just like solving the problem with first principles, understanding a hard topic from the first principles requires breaking it into sequence of simpler operations, and knowing exactly how data flows through these sequences. Which let you understand it inside out or comprehensively.



How I would learn any complex subjects like deep (machine??) learning and maths?
I would learn about any method from deep (machine??) learning and high level maths as a tool. I would just learn how that method can be used in the scenarios I am aware of or and not learn it comprehensively (knowing the full power and limitations of it). Let me provide two concrete examples of me learning about MCMC (Maths) and Batch Normalization (Deep Learning). I learned MCMC because it was getting used in Boltzmann Machine and Hopfield Network papers. I learned MCMC just upto an level so that I can understand both the papers. After understanding these two papers, I jumped to the next paper - Restricted Boltzmann Machine (RBM). In RBM, MCMC was being used in a way I though was not possible.
It was used in a parallel way while based on my understanding it could only be used in a sequential manner. So clearly I didn't understood it completely. I had only understood it as a tool for those two papers, where it could be only used as in sequential form. At first, it really surprised me but when I again tried to understand the MCMC in a more detailed manner, then I could understand its parallel use in RBM. So I was only learning MCMC as a tool for the selected papers. I am sure that there is so much more capabilities of MCMC, I am yet to discover.

Another example is Batch Normalization. When I learned about Batch Normalization, I understood that why it was proposed and how it can be useful. It was proposed to make the training of neural networks stable and faster. It seemed that it would become a standard layer in a neural network and that I should use it everytime I build a neural network. But later I came to know that there are some disadvantages and it is not a standard layer. Which is quite the opposite of my understanding. That is the problem with tool based understanding, one understand how a method is useful for a particular scenario. There is no comprehensive understanding which would include all the limitations and advantages of that method. 

I would only have an tool based understanding because of weak maths fundamentals. And tool based understanding is not transferable (or not scalable). If you understand one method (in whatever limited way because of weak maths fundamentals), it does not make you better at understanding the other methods. 

But if you have a strong maths fundamentals. Then not only it allows you to have a comprehensive understanding of a particular method but allows you to have a comprehensive understanding of any method. It is that powerful. Also comprehensive understanding of methods also makes the general skill of problem solving better. Because comprehensive understanding would make one learn, how to identify a problem and how different fundamental concepts can be used to create a method. It would help one to create new methods or improve the existing ones. It would make one a good researcher. 

Also, one is tend to forget the tool based understanding because it is based on shaky foundation. After some time, when one tries to recall the understanding, he has to recall the source or paper from which he understood it. How knowledge, experiments and observations of the method is presented in the paper will only help to recall the understanding. So, for every methods, you have to recall their respective sources, which does not sound easy. A good memory is required.  

With comprehensive understanding (with the help of strong maths fundamentals), you would not forget any method easily - be that method from deep (machine??) learning or from high level maths. At some later time, even if you do not remember the source from which you understood it, you can try to re-create the method on the fly. Recreation requires just mixing the maths fundamentals in a right manner and a problem solving skill. If one vaguely remembers the source, then it would also help. Ability to not forget easily, only makes one a better researcher. He can bring together concepts from already learned methods easily to create a new one or improve existing ones. 


So with strong maths fundamentals, one can become powerful?? where you can have comprehensive understanding of any method, ability to improve any method, to create new ones and to recall easily the already understood ones. You become a powerful researcher. And as you keep on comprehensively understand the existing methods, your research or problem solving skill only improves. And you only become powerful??, unlike in tool-based understanding where understanding a method is a burden because you have to remember the source.


All this depends on strong maths fundamentals. Learning maths fundamentals properly is not easy, in the following section, I elaborate on this. But if you have mastered the maths fundamentals, then you won't forget it easily. It is like one time investment, and in return, you remain powerful?? (in the sense explained above) forever and getting even more powerful?? as you move ahead in your research (ofcourse subject to the good functioning of bodily hardware). One time investment means with time, if exact details gets blurred out, you never have to spend same amount of time, you spent while mastering the fundamentals. A **quick revision** will make you the master again. Before futher explanation, let me clear what I meant by "maths fundamentals": maths knowledge, one requires to "easily" understand a high level maths or deep (machine??) learning method. For machine learning, maths fundamentals are Linear Algebra, Probability, Statistics, Multivariate Calculus and sometimes including other maths concepts. In school, we just get introduced to these topics along with other basic topics and learn them in detail in the college. But school level maths is fundamental for ML maths fundamentals. But everyone will agree that school level maths is indeed a one time investment that is a quick revision (in case of exact details blurring out) will make you master again. It is too hard to completely become noob at school level maths. Why? I think because a good study of school level maths wake up the **creator** which is within ourselves. When we think of school level maths, we feel like we could have also created that knowledge, if we were faced with challenges. When we do quick revision, you recall it naturally, as if we are creating the knowledge on the fly, filling up all the gap which got blurred out with time. One reason, we feel like creator because school level maths is so easy that **common sense** starts to work (for now, common sense I think is a good explanation until a better explanation I come to know). After becoming the creator of school level maths, mastering ML maths fundamentals is also a one time investment. When you master ML maths fundamentals, you realize that the gap between school level maths and ML maths fundamentals is also a common sense. And common sense always remain with you. So whenever details of ML maths fundamentals gets blurred out, with quick revision, your common sense will start to work and you will be able to quickly make the blurred out details crisp again.

## Expert Researcher = Production Line Expert
After my many iterations of going over maths and reading the papers, I have at least began to understand what is even meant by being a great researcher.
I have an analogy that works perfectly (acc. to my current understanding): The one with strong maths fundamentals?? is like an [production line](https://en.wikipedia.org/wiki/Production_line) expert, who has the knowledge of every production machine any production line (be it for potato chips, tyres, pencil etc.) can have. A production machine is a machine which does a **particular simple job** like cooling, mixing, boiling, cutting, steaming, shaping, fermentation, centrifugation, printing, packing etc. one can imagine a factory would have. A production line expert **knows in-and-out** how a particular production line is working. Which raw materials it can accept and what products it can produce. How each production machine is converting the input. Knows how material is getting transformed through out the production line. He knows how to **modify** the production line, if some different raw materials are also need to be accepted. Or can **improve it**, making it more efficient. He can even **create a new** production line for a different product from scratch. 

You might have already understood via the analogy. Here is the further clarification, just in case. The one with strong maths fundamentals?? is compared to the production line expert. Production machine is compared to simple, easy to understand, basic maths operations like plus, multiply etc. This operations are so simple, that they run in your blood already. A production line is compared to any high level maths or deep (machine??) learning method. Raw materials are compared to the input to these methods. And product produced by production line is the output of the method.

With strong maths fundamentals, one should be able to break *any* high level maths or deep (machine??) learning method into *simple operations* like plus, multiply etc. When the method is at work, you should know how an input is changed by every simple operations inside the method, how it is getting transformed into an output, as it runs through a sequence of simple operations forming a method. Like production line expert, it also allows you to modify any method for different scenario (different input) or allows you to improve any method or create a new method from scratch by combining simple operations. Hence, it allows you to become a researcher, a good researcher.


I have two more analogies but they are not as perfect as the production line. But they indeed tell you what it feels like having a strong maths fundamentals, if not then just ignore this para. First one: Being good at maths is like when waters flows through a straight pipe, you would know how every molecule might be flowing in a streamlined motion (or laminar flow) and you have no doubt that it might be moving otherwise. I feel like a a number inside the complex input like vector, is like a water molecule. One with good maths fundamentals should know how it is flowing inside the method. Second one: It is like those inspirational, redemption movies (based on art, sports etc), where a character transforms himself/herself from nothing in different environments as the movie progresses and come out as a winner at the end of a movie. A complex method have the same spirit, transforming the input at every step inside the method and outputting the transformed input. As one can easily understand such movies, in the same way one should understand any high level maths. 


---------
## Blogging
Why I could not learn the maths then?
It requires the ability to concentrate for long enough and hard work. I could not concentrate for longer. To put it simply, to concentrate is difficult because there is only one way to succeed while there are thousand ways to fail. There are thousand ways your mind can wander away while you are trying to concentrate on one topic. With concentration, one works efficiently and achieves the "top speed". Learning maths properly with your top speed would still take some significant amount of time but it will surely be fun in this way. just like when one enjoys driving at high speed and when you tap into the flow state (https://youtube.com/shorts/2zaeRz918Vs?si=ELfFPXOE2MJBv-DR). So when one is young, he could also naturally concentrate and also have enough time. Learning maths properly without your top speed, one would simply give up by observing the rate at which one is moving and distance left to covered. Speed is less, so time required would be more. There is also a factor of quality. One will have the understanding of high quality when one learns with concentration (won't hear the ticking clock, goes to a timeless zone). On the other hand, one is more susceptible (because clock is ticking) to take shortcuts in understanding when learning with lack of concentration (not to say that quality understanding can not be achieved, it's just that it will take a lot of time).

So, to resolve the contradiction, I need to learn maths but it is inherently slow. To learn it at a best speed, I need to develope concentration. Concentration can be developed with practice and discipline. But until you develop the concentration and also maintaining the quality understanding, learning process is slow and boring (laborous, against the resistance, not seamless). I don't wanna compromise the quality of my understanding. Then thoughts starts to arise about when I would be useful and be ready for service.

Answer to this is "Blogging". It solves both the problems - quality learning and service. If you are sharing some information, then it is better for oneself and others, that shared information is best to ones capacity. One would feel like doing a service, because there would at least be few persons in the world, you would love the content and would get benefitted.



Further Topics:
- Our true self is peaceful and blissful. That is, we are at this moment peaceful and blissful.And it will take some time (not years) to "regain" "realize"
- Along with strong maths fundamentals, there is other equally important skill which is required for a research -  is the ability make the "maths work inside the computer". Whatever maths we know, we know it so that we can make the computer intelligent. On paper, doing maths is easy (subject to strong maths fundamentals) but to make it work on computer requires different skills. Making it work on computer comes with "different" set of challenges which can not be solved by just having strong maths fundamentals. Challenges are like this. On paper, you can assume a variable as real and can solve it perfectly because there is no lose of precision. But on computer, a real number is stored only upto a finite precision and not infinite. Or I should better put it, challenge is to do continuous mathematics with discrete numbers. Field of Numerical Linear Algebra is one example of the field that deals with this problem. Other challenge includes the time. When we do maths on paper, we have the power to find that to what value a given sequence will converge to, by taking the limit. 
  There are some computations which we need, which is time taking on paper but fast for computers. To do such computations on computer, we should be able to "completely track the computations". We should know whether computer could do that computation in a more better way or not. Or computed values is the best ones or not. Two examples of such computations are sampling from high dimensional probability distributions and optimization of deep learning models.

- Having state the importance of mathematics and blogging. You have already guessed that I will be writing blogs on mathematics. I will start by Linear Algebra. Earlier, I would say that I wanna master AI. **But now my sole focus is to master Linear Algebra for Machine Learning**. After mastering LA for ML, I will start with mastering Probability and Statistics, Optimization etc. I haven't joined the university to become a good researcher because for joining a good one, you should first able to prove that you can handle the hard curriculam. I have not proved yet to myself, so how can I provide the proof to the university. Putting this out in public, makes me feel like I am enrolling to an university where you have to follow a curriculum and dropping out of it won't be easy and would take some strength. Also, this blog will be useful to others who are facing the same contradiction.

- AI is maths + data.
- I didn't talked about the middle ground of code-way and math-way. I only talked about the extremes.
- Blogging on AI maths fundamentals is an indirect way to contribute to AI.
  - Because if someone gets benefitted from it and becomes a researcher.

- it is not like mastering maths will take me forever, there is finite syllabus which will atleast allow you to comprehensively understand existing algorithms.

- I have deeeeeep desire to understand completely every papers published until now in the field of narrow or general AI. And maths give me that handle to atleast have such desire.

- I am not rigourous in terms of thinking about any concept. But I have to be rigorous. E.g. for the question, why training set should be shuffled? I never thought in this more rigorous manner: https://datascience.stackexchange.com/questions/24511/why-should-the-data-be-shuffled-for-machine-learning-tasks

- **My high level goal is to develope concentration and to understand any difficult topic easily.** 
  I "was" lazy. Now I have started my journey of blogging my way to become a production line expert of AI based production lines.

