---
title: Setting the tone and Statistical Mechanics, Hopfield Network and Boltzmann Machine
layout: post
subtitle: Contain links to the draft of three blogs, little bit of my journey and importance of maths.
---

When I started learning "Machine Learning" in the summer of 2019, I kept learning it since then because I find it very interesting. I learned it using the online resources. And after some couple of months, I got introduced to "Deep Learning" which made me realized that researchers are trying to mimic the human brain, so that computer can do human-level tasks. That idea of giving the human-level intelligence to computers is crazy. Also the idea that one solution can impact the diverse social problems like healthcare, education, agriculture, improving science etc. Since then I kept learning about deep learning, keeping aside the classical ML algorithms. Until now, I worked as ML engineer for 2 years and worked on exciting projects during my job and as a student. 

Mathematics is the language by which we (humans) communicate "How to be intelligent?" to the computer. Intelligence in simpler terms is processing the input (for example, images, text etc.) and based on this input, previous knowledge and the context of the input, take appropriate actions. Computer can only represents the data and knowledge in the form of numbers and can only manupulate the numbers, so that's where the maths equations come into the picture. By the help of human provided maths equations, it can become intelligent i.e. abot to extract the knowledge in the form of numbers (or at least narrow intelligence, i.e. only able to solve a particular problem and not able to solve any arbitrary problem) and then take appropriate actions. Without the mathematical knowledge, how one can communicate the computer ahout how to manipulate numbers to simulate intelligence. Or how a non-mathematician would able to understand that how exactly the scientist has made this AI model intelligent.

But all this time, I was clearly ignoring the mathematics, only relying on whatever mathematics I had learned in my school and first year of undergraduate. I was ignoring it because I was simply lazy to learn it (later I would elaborate why learning maths would make one lazy). And I would try to make my way through my job by just assuming about how already build AI models are working. So only had a half-baked knowledge of existing AI algorithms and surely won't be able to create new AI algorithms or improve the existing ones. Hence, I could not afford to be an AI researcher, the role I aspire for. Because it mainly deals with the questions "How exisiting AI models can be improved in terms of intelligence?" and not "How existing AI models can be implemented to use them at scale?". Implementing the AI system requies one to also work as a software engineer, which I am not interested in. Pushing the boundary of AI, finding new stuff, improve or better unerstand existing algorithms, that's the work I would like to do full time.

Here is the contradiction: **I find the idea of Artificial Intelligence so interesting and want to master it but I am also not trying to master the required mathematics**. I like AI so much that I never think of switching the field or job role. So to resolve this contradiction, I need to make the mathematics part of the contradiction to align with my interest. 

From time to time, I tried to resolve this contradiction by learning mathematics but couldn't learn it properly. After every iteration of me studying mathematics, I would end up with little more knowledge of mathematics but it would not be enough. 

Events (ChatGPT becoming popular) happened such that now I am working full time to resolve the contradiction. My first attempt was to again iterate over the mathematics but this time, in a systematic manner. I went through Linear Algebra, Probability, Statistics and Calculus. After spending the longest time on mathematics, I tried to test my knowledge by trying to understand the current research.

After looking at the open problems, I came to know the limitations of deep learning like robustness, continual learning, high sample complexity, interpretability etc. Since computer vision is my favourite among other subfields of AI, so looking at the potential solutions accordingly, lead me to compositionality and causality. And research of Francesco Locatello, Yoshua Bengio, Bernhard Scholkopf, Geoffrey Hinton and others. Surely I am not good enough _yet_ to properly understand their ongoing research. So, I thought to trace back their research. And find the sequence of key developements which will lead me to the ongoing research. This lead me to Boltzmann Machine (BM)(Hinton et al, 1985) and Hopfield Network (HN)(Hopfield, 1982). BM built upon the ideas of HN and both uses the ideas of Statistical Mechanics. The goal of BM was so ambitious as it tried to solve a broader problem of computer vision - classification, completing the partial image, imagination and understanding the content of the image. In theory, it could do all that but it was slow and not scalable. In 2006, Hinton showed that RBM (a form of BM - Restricted BM) can be used to train "deep" neural network in a better way (before this paper, training deep network was very difficult) and it gave new hope to the deep learning research community. 

The idea of tracing back and finding an older but key paper is good because when understood, it gives better understanding of multiple key developements based on it. Also, since it would be an older paper, so I thought it won't be too hard to understand (less high level concepts and closer to fundamentals). I traced back upto BM. I later came to know that BM is a good starting point, as it is also an important developement in terms of deep learning research. Because of this paper, there was a revival of deep learning in mid 2000's. Fun Fact: In [September 2024](https://github.com/joshi98kishan/deciphering-hopfield-network/blob/1475cb7d8a679f91cf78e58c8570166700383dff/README.md){:target="_blank"}, I decided to study Hopfield Network and Boltzmann Machine  and on [8th october, 2024](https://www.nobelprize.org/prizes/physics/2024/prize-announcement/){:target="_blank"}, Noble Prize in Physics was announced and it was given to the creators of these two models.

To understand BM, I had to understand HN and some simple concepts of Statistical Mechanics (field of Physics). This is for the first time, I took a paper seriously and tried to understand _every_ part of it. Full understanding comes when you see it working, so I implemented them too. This lead me to write three blogs on BM, HN and Statistical Mechanics (Stat Mech). Because as we know, in order to explain something _properly_, one should better know it. So explaining by writing helped my understanding. There were multiple experiments in both the paper (HN and BM) to show the capabilities of each technique. While there were no exact details about how those experiments are implemented, I implemented them based on my understanding. For HN experiments, there was a very slight difference between the results I obtained vs results mentioned in the paper. For BM, I was able to perfectly achieve all (total three) the experiment's results except for the third one. I was inefficient in terms of finding the right set of hyperparameters of the algorithm, it took me a lot of time and I could not spare more time for the third experiment.

The blogs are almost in their prefinal versions. Here is the notion link of each blog. And code for HN and BM:
- [Blog-Thermodynamics-Statistical Mechanics-Ising Model-AI](https://www.notion.so/Blog-Thermodynamics-Statistical-Mechanics-Ising-Model-AI-2bfa87ef96fd4cbfba09b493a6250cb2?source=copy_link)
- [Blog - Hopfield Network](https://www.notion.so/Blog-Hopfield-Network-1202f5954fbf80ac9f06fa5929c14493?source=copy_link) ([Code](https://github.com/joshi98kishan/deciphering-hopfield-network))
- [Blog - Boltzmann Machine](https://www.notion.so/Blog-Boltzmann-Machine-1202f5954fbf808ea6d8de1cfee7b30b?source=copy_link) ([Code](https://github.com/joshi98kishan/boltzmann-machine))

All these blogs are little messy and are 90% complete. You would find the grey colored text redundant. Unique selling point of these blogs is that I tried to make them self-contained. And there is complete, clean derivations of required equations in HN and BM.
I haven't completed them because it took me a lot of time reproducing the results. Since the writing part and experiments were almost done (except the last one), and I was not sure how much time the last experiment would take, so I felt like moving to the next key developement - RBM. 

While working on RBM, it occurred to me that I am very inefficient in terms of understanding the paper (taking long time to properly understand them). So, it would take me longer time to reach to current research. And that's my latest, strongest motivation for learning the maths. Why strongest? Because I had just spent 2 full months learning the maths and still I am inefficient to understand the papers.

 
After spending some time thinking about it. I just started to understand how should I understand the maths. I should understand the maths not just from the level of equations but I have to go more deeper until I break the maths concept into basic operations of plus, minus, multiply and divide. 

Learning maths is difficult because there so many layers of abstractions. Until you  







How I would learn any complex subjects like deep learning and maths?
I would learn about any method from deep learning and high level maths as a tool. I would just learn how that method can be used in the scenarios I am aware of or and not learn it comprehensively (knowing the full power and limitations of it). Let me provide two concrete examples of me learning about MCMC (Maths) and Batch Normalization (Deep Learning). I learned MCMC because it was getting used in Boltzmann Machine and Hopfield Network papers. I learned MCMC just upto an level so that I can understand both the papers. After understanding these two papers, I jumped to the next paper - Restricted Boltzmann Machine (RBM). In RBM, MCMC was being used in a way I though was not possible.
It was used in a parallel way while based on my understanding it could only be used in a sequential manner. So clearly I didn't understood it completely. I had only understood it as a tool for those two papers, where it could be only used as in sequential form. At first, it really surprised me but when I again tried to understand the MCMC in a more detailed manner, then I could understand its parallel use in RBM. So I was only learning MCMC as a tool for the selected papers. I am sure that there is so much more capabilities of MCMC, I am yet to discover.

Another example is Batch Normalization. When I learned about Batch Normalization, I understood that why it was proposed and how it can be useful. It was proposed to make the training of neural networks stable and faster. It seemed that it would become a standard layer in a neural network and that I should use it everytime I build a neural network. But later I came to know that there are some disadvantages and it is not a standard layer. Which is quite the opposite of my understanding. That is the problem with tool based understanding, one understand how a method is useful for a particular scenario. There is no comprehensive understanding which would include all the limitations and advantages of that method. 

I would only have an tool based understanding because of weak maths fundamentals. And tool based understanding is not transferable (or not scalable). If you understand one method, it does not make you better at understanding the other methods. 

But if you have a strong maths fundamentals. Then not only it allows you to have a comprehensive understanding of a particular method but allows you to have a comprehensive understanding of any method. It is that powerful. Also comprehensive understanding of methods also makes the general skill of problem solving better. Because comprehensive understanding would make one learn, how to identify a problem and how different fundamental concepts can be used to create a method. It would help one to create new methods or improve the existing ones. It would make one a good researcher. 

Also, one is tend to forget the tool based understanding because it is based on shaky foundation. After some time, when one tries to recall the understanding, he has to recall the source or paper from which he understood it. How knowledge, experiments and observations of the method is presented in the paper will only help to recall the understanding. So, for every methods, you have to recall their respective sources, which does not sound easy. A good memory is required.  

With comprehensive understanding (with the help of strong maths fundamentals), you would not forget any method easily - be that method from deep learning or from high level maths. At some later time, even if you do not remember the source from which you understood it, you can try to re-create the method on the fly. Recreation requires just mixing the maths fundamentals in a right manner and a problem solving skill. If one vaguely remembers the source, then it would also help. 


So with strong maths fundamentals, one can become powerful where you can have comprehensive understanding of any method, ability to improve any method, to create new ones and to recall easily the already understood ones. You become a powerful researcher. And as you keep on comprehensively understand the existing methods, your research or problem solving skill only improves. And you only become powerful, unlike in tool-based understanding where understanding a method is a burden because you have to remember the source.


All this depends on strong maths fundamentals. Learning maths fundamentals properly is not easy, in the following section, I elaborate on this. But if you have mastered the maths, then you won't forget it easily. It is like one time investment, and in return, you remain powerful (in the sense explained above) forever and getting even more powerful as you move ahead in your research (subject to the good functioning of bodily hardware). After my many iterations of going over maths, I have at least understood what is meant by strong maths fundamentals. 

Now the question arises how would you remember your maths fundamentals? or how would you learn them?

   In the context of AI, fundamentals are LA, prob, stats, calculus and sometimes including other maths concepts. But this maths subjects are based on THE fundamentals of maths - numbers and basic operations like add, substract and they are so hard to forget (HAHA!).  So if you have understood this subject fundamentally then you won't forget it.

---------

Why I could not learn the maths then?
It requires the ability to concentrate for long enough and hard work. I could not concentrate for longer. To put it simply, to concentrate is difficult because there is only one way to succeed while there are thousand ways to fail. There are thousand ways your mind can wander away while you are trying to concentrate on one topic. With concentration, one works efficiently and achieves the "top speed". Learning maths properly with your top speed would still take some significant amount of time but it will surely be fun in this way. just like when one enjoys driving at high speed and when you tap into the flow state (https://youtube.com/shorts/2zaeRz918Vs?si=ELfFPXOE2MJBv-DR). So when one is young, he could also naturally concentrate and also have enough time. Learning maths properly without your top speed, one would simply give up by observing the rate at which one is moving and distance left to covered. Speed is less, so time required would be more. There is also a factor of quality. One will have the understanding of high quality when one learns with concentration (won't hear the ticking clock, goes to a timeless zone). On the other hand, one is more susceptible (because clock is ticking) to take shortcuts in understanding when learning with lack of concentration (not to say that quality understanding can not be achieved, it's just that it will take a lot of time).

So, to resolve the contradiction, I need to learn maths but it is inherently slow. To learn it at a best speed, I need to develope concentration. Concentration can be developed with practice and discipline. But until you develop the concentration and also maintaining the quality understanding, learning process is slow and boring. I don't wanna compromise the quality of my understanding. Then thoughts starts to arise about when I would be useful and be ready for service.

Answer to this is "Blogging". It solves both the problems - quality learning and service. If you are sharing some information, then it is better for oneself and others, that shared information is best to ones capacity. One would feel like doing a service, because there would at least be few persons in the world, you would love the content and would get benefitted.



  While one is improving the concentration and currently learning slowly. To motivate oneself


Topics:
- Our true self is peaceful and blissful. That is, we are at this moment peaceful and blissful.And it will take some time (not years) to "regain" "realize"
- I haven't joined the university to become a good researcher because for joining a good one, you should first able to prove that you can handle the hard curriculam. I have not proved yet to myself, so how can I provide the proof to the university. 
- Along with strong maths fundamentals, there is other equally important skill which is required for a research -  is the ability make the "maths work inside the computer". Whatever maths we know, we know it so that we can make the computer intelligent. On paper, doing maths is easy (subject to strong maths fundamentals) but to make it work on computer requires different skills. Making it work on computer comes with "different" set of challenges which can not be solved by just having strong maths fundamentals. Challenges are like this. On paper, you can assume a variable as real and can solve it perfectly because there is no lose of precision. But on computer, a real number is stored only upto a finite precision and not infinite. Or I should better put it, challenge is to do continuous mathematics with discrete numbers. Field of Numerical Linear Algebra deals with this problem. Other challenge includes the time